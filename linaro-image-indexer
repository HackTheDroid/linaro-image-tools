#!/usr/bin/env python
# Copyright (C) 2010, 2011 Linaro
#
# Author: James Tunnicliffe <james.tunnicliffe@linaro.org>
#
# This file is part of Linaro Image Tools.
#
# Linaro Image Tools is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
# 
# Linaro Image Tools is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with Linaro Image Tools; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301,
# USA.

import os
import re
import urlparse
import logging
import bz2
import linaro_image_tools.fetch_image

RELEASES_WWW_DOCUMENT_ROOT  = "/home/dooferlad/dev/to_index"
RELEASE_URL                 = "http://releases.linaro.org/platform/"
SNAPSHOTS_WWW_DOCUMENT_ROOT = "/srv/snapshots.linaro.org/www/"
SNAPSHOTS_URL               = "http://snapshots.linaro.org/"

# Log everything, and send it to stderr.
logging.basicConfig(level=logging.DEBUG)

class ServerIndexer():
    """Create a database of files on the linaro image servers for use by image
       creation tools."""
    def reset(self):
        self.url_parse = {}

    def __init__(self):
        self.reset()
        self.db_file_name = "server_index"
        self.db = linaro_image_tools.fetch_image.DB(self.db_file_name)

    def regexp_list_matches(self, to_search, list):
        if len(list) == 0:
            return True

        for item in list:
            if re.search(item, to_search):
                return True

        return False

    def crawl(self):
        self.db.set_url_parse_info(self.url_parse)
        logging.info(self.url_parse.items())
        
        for table, info in self.url_parse.items():
            logging.info("------------------------------------------")
            logging.info("{0}: {1}".format(table, info['db_columns']))
            logging.info("{0} {1} {2} {3}".format(
                          info["base_dir"], info["base_url"],
                          info["url_validator"], info["url_chunks"]))
            self.go(info, table)

    def go(self, info, table):
        root_url = info["base_url"]
        root_dir = info["base_dir"]

        for root, subFolders, files in os.walk( root_dir ):
            for file in files:
                relative_location = re.sub(root_dir, "",
                                           os.path.join(root, file))
                relative_location = relative_location.lstrip("/")
                
                to_match = info["url_validator"][0]
                not_match = info["url_validator"][1]

                if(not (self.regexp_list_matches(relative_location, to_match)
                        and not self.regexp_list_matches(relative_location,
                                                         not_match))
                   or not re.search("\.gz$", file)):
                    continue  # URL doesn't match the validator. Ignore.

                url = urlparse.urljoin(root_url, relative_location)
                url = urlparse.urljoin(url, file)

                logging.info("{0}: {1}".format(table, url))
                self.db.record_url(url, table)
                    
        self.dump() 

    def dump(self):
        self.db.commit()
        
    def close_and_bzip2(self):
        # After finishing creating the database, create a compressed version
        # for more efficient downloads
        self.db.close()
        bz2_db_file = bz2.BZ2File(self.db_file_name + ".bz2", "w")
        db_file = open(self.db_file_name)
        bz2_db_file.write(db_file.read())
        bz2_db_file.close()

    def add_directory_parse_list(self,
                                 base_dir,
                                 base_url,
                                 url_validator,
                                 db_columns,
                                 id,
                                 url_chunks):
        
        if not id in self.url_parse:
            self.url_parse[id] = {"base_dir": base_dir,
                                   "base_url": base_url,
                                   "url_validator": url_validator,
                                   "db_columns": db_columns,
                                   "url_chunks": url_chunks}
            logging.info(self.url_parse[id]["base_dir"])

            # Construct data needed to create the table
            items = []
            for item in url_chunks:
                if(item != ""):
                    # If the entry is a tuple, it indicates it is of the
                    # form name, regexp
                    if(isinstance(item, tuple)):
                        items.append(item[0])
                    else:
                        items.append(item)

            self.db.create_table_with_name_columns(id, db_columns)

    def clean_removed_urls_from_db(self):
        self.db.clean_removed_urls_from_db()

if __name__ == '__main__':
    crawler = ServerIndexer()

    old = False

    if old:
        #releases.linaro.org/platform/linaro-m/plasma/final/
        crawler.add_directory_parse_list(RELEASES_WWW_DOCUMENT_ROOT,
                                         RELEASE_URL,
                                         ([], ["/platform/", "/old/", "hwpack"]),
                                         ["platform", "image", "build"],
                                         "release_binaries",
                                         ["platform", "image", "build"])
    else:
        # 11.06/ubuntu/panda
        crawler.add_directory_parse_list(
                    RELEASES_WWW_DOCUMENT_ROOT,
                    RELEASE_URL,
                    ([], ["/latest/", "/platform/", "/old/", "hwpack"]),
                    ["platform", "image", "build"],
                    "release_binaries",
                    ["platform", "image", ""])

    if old:
        #releases.linaro.org/platform/linaro-m/hwpacks/final/
        # hwpack_linaro-bsp-omap4_20101109-1_armel_unsupported.tar.gz
        crawler.add_directory_parse_list(RELEASES_WWW_DOCUMENT_ROOT,
                                         RELEASE_URL,
                                         (["/hwpacks/"], ["/platform/"]),
                                         "release_hwpacks",
                                         ["plaform", "build", "hardware"],
                                         ["platform", "", "build",
                                          ("hardware", r"hwpack_linaro-(.*?)_")])
    else:
        # 11.06/ubuntu/panda
        crawler.add_directory_parse_list(
                  RELEASES_WWW_DOCUMENT_ROOT,
                  RELEASE_URL,
                  (["hwpack"], ["/platform/"]),
                  ["platform", "build", "hardware"],
                  "release_hwpacks",
                  ["platform", "", "", ("hardware", r"hwpack_linaro-(.*?)_")])
#
#    #http://snapshots.linaro.org/11.05-daily/linaro-alip/20110420/0/images/tar/
#    crawler.add_directory_parse_list(SNAPSHOTS_WWW_DOCUMENT_ROOT,
#                                     SNAPSHOTS_URL,
#                                     r"^((?!hwpack).)*$",
#                                     "snapshot_binaries",
#                                     ["platform", "image", "date", "build"])
#
#    #http://snapshots.linaro.org/11.05-daily/linaro-hwpacks/omap3/20110420/0/images/hwpack/
#    crawler.add_directory_parse_list(SNAPSHOTS_WWW_DOCUMENT_ROOT,
#                                     SNAPSHOTS_URL,
#                                     r"/hwpack/",
#                                     "snapshot_hwpacks",
#                                     ["platform", "", "hardware", "date",
#                                      "build"])

    crawler.crawl()
    crawler.clean_removed_urls_from_db()
    crawler.dump()
    crawler.close_and_bzip2()
